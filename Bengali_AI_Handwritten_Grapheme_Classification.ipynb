{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bengali_AI_Handwritten_Grapheme_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1qEQfueB93hNSGqyiToQzEV7DhqENUinh",
      "authorship_tag": "ABX9TyPZn0ev2pSMDWO7shdYbwRk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simoncuet/Assistive-Application-for-Visually-Impaired-Children/blob/master/Bengali_AI_Handwritten_Grapheme_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaiAEoBphPuZ",
        "colab_type": "code",
        "outputId": "ec30d364-aaca-49d6-c67e-b271de1f5f62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import gc\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pylab as plt\n",
        "from tensorflow import keras\n",
        "from keras import applications\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras.models import Model, load_model\n",
        "from tensorflow.keras import layers\n",
        "from keras.initializers import glorot_uniform\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Connect to my google drive where train and test data are located\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DwTjO0gSDZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_purquet = 4 # both for test and train images\n",
        "img_w, img_h = 137, 236\n",
        "resize_w, resize_h = 64, 64\n",
        "\n",
        "# train data\n",
        "train_csv_path = '/content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train.csv'\n",
        "train_img_path = []\n",
        "for i in range(num_purquet):\n",
        "  train_img_path.append(f'/content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train_image_data_{i}.parquet')\n",
        "\n",
        "# test data\n",
        "test_csv_path = '/content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/test.csv'\n",
        "test_img_path = []\n",
        "for i in range(num_purquet):\n",
        "  test_img_path.append(f'/content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/test_image_data_{i}.parquet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XkF2MD5J3JH",
        "colab_type": "code",
        "outputId": "6d427591-312d-4c4a-e10a-7822d4caa270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "train_label = pd.read_csv(train_csv_path)\n",
        "train_data_list = []\n",
        "y_root = []\n",
        "y_vowel = []\n",
        "y_consonant = []\n",
        "for file_path in train_img_path:\n",
        "  resized = {}\n",
        "  df =  pd.merge(pd.read_parquet(file_path), train_label, on='image_id').drop(['image_id'], axis =1)\n",
        "  print(f'data loaded from {file_path}')\n",
        "  y_root.append(df['grapheme_root'].values)\n",
        "  y_vowel.append(df['vowel_diacritic'].values)\n",
        "  y_consonant.append(df['consonant_diacritic'].values)\n",
        "  df = df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic','grapheme'], axis=1)\n",
        "  for i in range(df.shape[0]):\n",
        "    if i%10000==0:\n",
        "      print(i)\n",
        "    img = cv2.resize(df.loc[df.index[i]].values.reshape(img_w,img_h),(resize_w,resize_h))\n",
        "    #img=img/255\n",
        "    resized[df.index[i]] = img.reshape(-1)\n",
        "  resized = pd.DataFrame(resized).T\n",
        "  train_data_list.append(resized)\n",
        "  del df\n",
        "  gc.collect()\n",
        "x = pd.concat(train_data_list, ignore_index=True)\n",
        "del train_data_list\n",
        "gc.collect()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data loaded from /content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train_image_data_0.parquet\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "data loaded from /content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train_image_data_1.parquet\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "data loaded from /content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train_image_data_2.parquet\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "data loaded from /content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train_image_data_3.parquet\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Z86tholJys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_root = np.concatenate(y_root, axis = None)\n",
        "y_vowel = np.concatenate(y_vowel, axis = None)\n",
        "y_consonant = np.concatenate(y_consonant, axis = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUBgIvAHlmSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_test, y_root_train, y_root_test, y_vowel_train, y_vowel_test, y_consonant_train, y_consonant_test = train_test_split(x, y_root, y_vowel, y_consonant, test_size = 0.08, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rze3suMEAKKh",
        "colab_type": "code",
        "outputId": "59ef03d0-1021-44ca-d0a4-a6b83bc8aae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(x_train.shape, x_test.shape)\n",
        "print(y_root_train.shape, y_root_test.shape)\n",
        "print(y_vowel_train.shape, y_vowel_test.shape)\n",
        "print(y_consonant_train.shape, y_consonant_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(184772, 4096) (16068, 4096)\n",
            "(184772,) (16068,)\n",
            "(184772,) (16068,)\n",
            "(184772,) (16068,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmYLm0um7viw",
        "colab_type": "code",
        "outputId": "079e794b-242b-4489-bccf-341e7af51d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "del x\n",
        "del y_root\n",
        "del y_vowel\n",
        "del y_consonant\n",
        "gc.collect()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKFjeR-AC0fQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def identity_block(X, f, filters, stage, block):\n",
        "    \n",
        "    # Defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "    \n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1, 1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path \n",
        "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1, 1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X\n",
        "\n",
        "def convolutional_block(X, f, filters, stage, block, s=2):\n",
        "\n",
        "    # Defining name basis\n",
        "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    # First component of main path \n",
        "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '2a', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2a')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Second component of main path\n",
        "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same', name=conv_name_base + '2b', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2b')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Third component of main path\n",
        "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name=conv_name_base + '2c', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name=bn_name_base + '2c')(X)\n",
        "\n",
        "    ##### SHORTCUT PATH #### \n",
        "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid', name=conv_name_base + '1', kernel_initializer=glorot_uniform(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis=3, name=bn_name_base + '1')(X_shortcut)\n",
        "\n",
        "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "def ResNet50(input_shape = (64, 64, 3), classes = 6):\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input =Input(input_shape)\n",
        "\n",
        "    \n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    # Stage 3\n",
        "    X = convolutional_block(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = identity_block(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    # Stage 4\n",
        "    X = convolutional_block(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = identity_block(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    # Stage 5\n",
        "    X = convolutional_block(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = identity_block(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    # AVGPOOL\n",
        "    X = AveragePooling2D(pool_size=(2,2), padding='same')(X)\n",
        "\n",
        "    # Output layer\n",
        "    X = Flatten()(X)\n",
        "    model = Dense(1024, activation = \"relu\")(X)\n",
        "    #model = layers.Dropout(rate=0.3)(model)\n",
        "    dense = Dense(512, activation = \"relu\")(model)\n",
        "    #X = Dense(classes[0], activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    out_root = Dense(classes[0], activation='softmax', name='root_out') (dense)\n",
        "    out_vowel = Dense(classes[1], activation='softmax', name='vowel_out') (dense)\n",
        "    out_consonant = Dense(classes[2], activation='softmax', name='consonant_out')(dense)\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = [out_root, out_vowel, out_consonant])\n",
        "    model.compile(optimizer = tf.train.AdamOptimizer(), loss=['sparse_categorical_crossentropy', 'sparse_categorical_crossentropy', 'sparse_categorical_crossentropy'], metrics=['accuracy'])\n",
        "  \n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "    #model = Model(inputs = X_input, outputs = X, name='ResNet50')\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL2yjo-qDi8j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "69a74247-3662-4e22-cc5f-753fc8e5b7c7"
      },
      "source": [
        "model = ResNet50(input_shape = (64, 64, 1), classes = [168,11,7])\n",
        "history = model.fit(x_train.values.reshape(-1, resize_w, resize_h,1), {'root_out':y_root_train, 'vowel_out':y_vowel_train, 'consonant_out':y_consonant_train}, epochs=20, batch_size = 128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Epoch 1/20\n",
            " 61184/184772 [========>.....................] - ETA: 2:28:22 - loss: 7.3446 - root_out_loss: 4.6079 - vowel_out_loss: 1.6159 - consonant_out_loss: 1.1209 - root_out_acc: 0.0404 - vowel_out_acc: 0.4438 - consonant_out_acc: 0.6360"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9_dqKcf8DdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "  inputs = keras.Input(shape=(resize_w, resize_h,1))\n",
        "  #CONV 1A STARTS\n",
        "  model = layers.Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(inputs)\n",
        "  model = tf.nn.leaky_relu(model, alpha=0.01, name='Leaky_ReLU') \n",
        "  model = layers.Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
        "  model = tf.nn.leaky_relu(model, alpha=0.01, name='Leaky_ReLU') \n",
        "  model = layers.BatchNormalization(momentum=0.15)(model)\n",
        "  model = layers.MaxPool2D(pool_size=(2, 2))(model)\n",
        "  model = layers.Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
        "  model = tf.nn.leaky_relu(model, alpha=0.01, name='Leaky_ReLU') \n",
        "  model = layers.Dropout(rate=0.3)(model)\n",
        "  #CONV 1B STARTS\n",
        "  model = layers.Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(inputs)\n",
        "  model = tf.nn.leaky_relu(model, alpha=0.01, name='Leaky_ReLU') \n",
        "  model = layers.Conv2D(filters=32, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
        "  model = tf.nn.leaky_relu(model, alpha=0.01, name='Leaky_ReLU') \n",
        "  model = layers.BatchNormalization(momentum=0.15)(model)\n",
        "  model = layers.MaxPool2D(pool_size=(2, 2))(model)\n",
        "  model = layers.Conv2D(filters=32, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
        "  model = tf.nn.leaky_relu(model, alpha=0.01, name='Leaky_ReLU') \n",
        "  model = layers.Dropout(rate=0.3)(model)\n",
        "  #CONV 2A STARTS\n",
        "  model = layers.Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
        "  model = tf.nn.leaky_relu(model, alpha=0.01, name='Leaky_ReLU') \n",
        "  model = layers.BatchNormalization(momentum=0.15)(model)\n",
        "  model = layers.Dropout(rate=0.3)(model)\n",
        "  #CONV 2B STARTS\n",
        "  model = layers.Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
        "  model = tf.nn.leaky_relu(model, alpha=0.01, name='Leaky_ReLU') \n",
        "  model = layers.BatchNormalization(momentum=0.15)(model)\n",
        "  model = layers.Dropout(rate=0.3)(model)\n",
        "  #CONV 3A STARTS\n",
        "  model = layers.Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
        "  model = tf.nn.leaky_relu(model, alpha=0.01, name='Leaky_ReLU') \n",
        "  model = layers.Conv2D(filters=64, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
        "  model = tf.nn.leaky_relu(model, alpha=0.01, name='Leaky_ReLU') \n",
        "  model = layers.BatchNormalization(momentum=0.15)(model)\n",
        "  model = layers.MaxPool2D(pool_size=(2, 2))(model)\n",
        "  #CONV 3B STARTS\n",
        "  model = layers.Conv2D(filters=64, kernel_size=(5, 5), padding='SAME', activation='relu')(model)\n",
        "  model = tf.nn.leaky_relu(model, alpha=0.01, name='Leaky_ReLU') \n",
        "  model = layers.BatchNormalization(momentum=0.15)(model)\n",
        "  model = layers.Dropout(rate=0.3)(model)\n",
        "  \n",
        "  model = layers.Flatten()(model)\n",
        "  model = layers.Dense(1024, activation = \"relu\")(model)\n",
        "  model = layers.Dropout(rate=0.3)(model)\n",
        "  dense = layers.Dense(512, activation = \"relu\")(model)\n",
        "\n",
        "  #Decision\n",
        "  out_root = layers.Dense(168, activation='softmax', name='root_out') (dense)\n",
        "  out_vowel = layers.Dense(11, activation='softmax', name='vowel_out') (dense)\n",
        "  out_consonant = layers.Dense(7, activation='softmax', name='consonant_out')(dense)\n",
        "\n",
        "  model = keras.Model(inputs = inputs, outputs = [out_root, out_vowel, out_consonant])\n",
        "  model.compile(optimizer = tf.train.AdamOptimizer(), loss=['sparse_categorical_crossentropy', 'sparse_categorical_crossentropy', 'sparse_categorical_crossentropy'], metrics=['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvhpG4479zHe",
        "colab_type": "code",
        "outputId": "a544837d-12ef-4f63-c3fe-14579649c2a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "model = get_model()\n",
        "history = model.fit(x_train.values.reshape(-1, resize_w, resize_h,1), {'root_out':y_root_train, 'vowel_out':y_vowel_train, 'consonant_out':y_consonant_train}, epochs=20, batch_size = 256)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 184772 samples\n",
            "Epoch 1/20\n",
            "157696/184772 [========================>.....] - ETA: 14:00 - loss: 5.1192 - root_out_loss: 3.5922 - vowel_out_loss: 0.8026 - consonant_out_loss: 0.7244 - root_out_acc: 0.1645 - vowel_out_acc: 0.7238 - consonant_out_acc: 0.7535"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-428691d70aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_h\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'root_out'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_root_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vowel_out'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_vowel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'consonant_out'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_consonant_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWXYa1TUCTwg",
        "colab_type": "code",
        "outputId": "7668fa03-4b85-4292-fa9a-347623d965da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "source": [
        "model.evaluate(x_test.values.reshape(-1, 64, 64, 1), {'root_out':y_root_test, 'vowel_out':y_vowel_test, 'consonant_out':y_consonant_test})\n",
        "\n",
        "#print('Test accuracy:', test_acc)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4017/4017 [==============================] - 7s 2ms/sample - loss: 2.1465 - root_out_loss: 1.5527 - vowel_out_loss: 0.2995 - consonant_out_loss: 0.2985 - root_out_acc: 0.5676 - vowel_out_acc: 0.8992 - consonant_out_acc: 0.9014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.1465109240987985,\n",
              " 1.5526849,\n",
              " 0.29947168,\n",
              " 0.29846987,\n",
              " 0.56758773,\n",
              " 0.8991785,\n",
              " 0.901419]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypPpaCIMpRsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_dict = {\n",
        "    'grapheme_root': [],\n",
        "    'vowel_diacritic': [],\n",
        "    'consonant_diacritic': []\n",
        "}\n",
        "\n",
        "components = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\n",
        "target=[] # model predictions placeholder\n",
        "row_id=[] # row_id place holder\n",
        "for i in range(4):\n",
        "    df_test_img = pd.read_parquet('../input/bengaliai-cv19/test_image_data_{}.parquet'.format(i)) \n",
        "    df_test_img.set_index('image_id', inplace=True)\n",
        "    X_test = resize(df_test_img, need_progress_bar=False)/255\n",
        "    X_test = X_test.values.reshape(-1, 64, 64, 1)\n",
        "    preds = cnn1model.predict(X_test)\n",
        "    for i, p in enumerate(preds_dict):\n",
        "        preds_dict[p] = np.argmax(preds[i], axis=1)\n",
        "\n",
        "    for k,id in enumerate(df_test_img.index.values):  \n",
        "        for i,comp in enumerate(components):\n",
        "            id_sample=id+'_'+comp\n",
        "            row_id.append(id_sample)\n",
        "            target.append(preds_dict[comp][k])\n",
        "    del df_test_img\n",
        "    del X_test\n",
        "    gc.collect()\n",
        "df_sample = pd.DataFrame({'row_id': row_id, 'target':target}, columns = ['row_id','target'])\n",
        "df_sample.to_csv('submission.csv',index=False)\n",
        "df_sample.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ_z7cLH_0ey",
        "colab_type": "code",
        "outputId": "7778697e-26e8-46f2-c7f0-d4694da1583d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4bc6a30f992c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'version' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieF6kmF5ACm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read train data into: df\n",
        "df1 = pd.read_parquet('/content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train_image_data_0.parquet')\n",
        "df2 = pd.read_parquet('/content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train_image_data_1.parquet')\n",
        "df3 = pd.read_parquet('/content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train_image_data_2.parquet')\n",
        "df4 = pd.read_parquet('/content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train_image_data_3.parquet')\n",
        "list_df = [df1, df2, df3, df4]\n",
        "df = pd.concat(list_df, ignore_index=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o8111XFpnsD",
        "colab_type": "code",
        "outputId": "87ef331c-d880-4f80-8221-2c4264787ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data = {'A': [1, 2, 3], 'B': [4, 5, 6], 'C': [7, 8, 9]}\n",
        "df = pd.DataFrame(data)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1ccecc5ffa29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'B'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubXAMUxDplnh",
        "colab_type": "code",
        "outputId": "2f870f11-acd5-46c5-b42d-3d9dfa6cbc16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "train_data = pd.read_csv('/content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train.csv')\n",
        "data_list = []\n",
        "for i in range(4):\n",
        "  resized = {}\n",
        "  print('data loaded..',i)\n",
        "  df =  pd.merge(pd.read_parquet(f'/content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train_image_data_{i}.parquet'), train_data, on='image_id').drop(['image_id'], axis =1)\n",
        "  df = df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic','grapheme'], axis=1)\n",
        "  for i in range(df.shape[0]):\n",
        "    if i%10000==0:\n",
        "      print(i)\n",
        "    #img = cv2.resize(df.loc[df.index[i]].values.reshape(137,236), (64,64))\n",
        "    img = df.loc[df.index[i]].values.reshape(137,236)\n",
        "    resized[df.index[i]] = img.reshape(-1)\n",
        "  resized = pd.DataFrame(resized).T\n",
        "  data_list.append(resized)\n",
        "df = pd.concat(data_list, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data loaded.. 0\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "data loaded.. 1\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "data loaded.. 2\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n",
            "data loaded.. 3\n",
            "0\n",
            "10000\n",
            "20000\n",
            "30000\n",
            "40000\n",
            "50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfLiFCJo9GMR",
        "colab_type": "code",
        "outputId": "2484f14c-d9de-48b6-e6fb-60bd70f22417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>32292</th>\n",
              "      <th>32293</th>\n",
              "      <th>32294</th>\n",
              "      <th>32295</th>\n",
              "      <th>32296</th>\n",
              "      <th>32297</th>\n",
              "      <th>32298</th>\n",
              "      <th>32299</th>\n",
              "      <th>32300</th>\n",
              "      <th>32301</th>\n",
              "      <th>32302</th>\n",
              "      <th>32303</th>\n",
              "      <th>32304</th>\n",
              "      <th>32305</th>\n",
              "      <th>32306</th>\n",
              "      <th>32307</th>\n",
              "      <th>32308</th>\n",
              "      <th>32309</th>\n",
              "      <th>32310</th>\n",
              "      <th>32311</th>\n",
              "      <th>32312</th>\n",
              "      <th>32313</th>\n",
              "      <th>32314</th>\n",
              "      <th>32315</th>\n",
              "      <th>32316</th>\n",
              "      <th>32317</th>\n",
              "      <th>32318</th>\n",
              "      <th>32319</th>\n",
              "      <th>32320</th>\n",
              "      <th>32321</th>\n",
              "      <th>32322</th>\n",
              "      <th>32323</th>\n",
              "      <th>32324</th>\n",
              "      <th>32325</th>\n",
              "      <th>32326</th>\n",
              "      <th>32327</th>\n",
              "      <th>32328</th>\n",
              "      <th>32329</th>\n",
              "      <th>32330</th>\n",
              "      <th>32331</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>...</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>251</td>\n",
              "      <td>244</td>\n",
              "      <td>238</td>\n",
              "      <td>245</td>\n",
              "      <td>248</td>\n",
              "      <td>246</td>\n",
              "      <td>246</td>\n",
              "      <td>247</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>246</td>\n",
              "      <td>249</td>\n",
              "      <td>248</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>...</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>245</td>\n",
              "      <td>247</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>...</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>251</td>\n",
              "      <td>249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>247</td>\n",
              "      <td>247</td>\n",
              "      <td>249</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>250</td>\n",
              "      <td>249</td>\n",
              "      <td>250</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>...</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>252</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>253</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>254</td>\n",
              "      <td>253</td>\n",
              "      <td>253</td>\n",
              "      <td>252</td>\n",
              "      <td>251</td>\n",
              "      <td>252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>249</td>\n",
              "      <td>248</td>\n",
              "      <td>246</td>\n",
              "      <td>246</td>\n",
              "      <td>248</td>\n",
              "      <td>244</td>\n",
              "      <td>242</td>\n",
              "      <td>242</td>\n",
              "      <td>229</td>\n",
              "      <td>225</td>\n",
              "      <td>231</td>\n",
              "      <td>229</td>\n",
              "      <td>229</td>\n",
              "      <td>228</td>\n",
              "      <td>221</td>\n",
              "      <td>224</td>\n",
              "      <td>226</td>\n",
              "      <td>221</td>\n",
              "      <td>221</td>\n",
              "      <td>220</td>\n",
              "      <td>217</td>\n",
              "      <td>217</td>\n",
              "      <td>218</td>\n",
              "      <td>219</td>\n",
              "      <td>222</td>\n",
              "      <td>224</td>\n",
              "      <td>214</td>\n",
              "      <td>218</td>\n",
              "      <td>227</td>\n",
              "      <td>227</td>\n",
              "      <td>227</td>\n",
              "      <td>228</td>\n",
              "      <td>224</td>\n",
              "      <td>231</td>\n",
              "      <td>235</td>\n",
              "      <td>235</td>\n",
              "      <td>233</td>\n",
              "      <td>212</td>\n",
              "      <td>183</td>\n",
              "      <td>196</td>\n",
              "      <td>...</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "      <td>255</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 32332 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0      1      2      3      4      ...  32327  32328  32329  32330  32331\n",
              "0    254    253    252    253    251  ...    253    253    253    253    251\n",
              "1    251    244    238    245    248  ...    255    255    255    255    254\n",
              "2    251    250    249    250    249  ...    253    253    253    251    249\n",
              "3    247    247    249    253    253  ...    253    253    252    251    252\n",
              "4    249    248    246    246    248  ...    255    255    255    255    255\n",
              "\n",
              "[5 rows x 32332 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIyd0c0KCPp5",
        "colab_type": "code",
        "outputId": "deceb402-97c9-465f-f2b4-e054284175ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "img_df = df.loc[3]\n",
        "img_arr = img_df.values\n",
        "img_arr = img_arr.reshape(137,236,1)\n",
        "img_arr.shape\n",
        "cv2.imwrite(\"hi.jpg\", img_arr)\n",
        "\n",
        "img = cv2.imread('hi.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "#img = cv2.resize(img, (225,225))\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-89562c62ae71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m137\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m236\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hi.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdRYRt7xnoVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split\n",
        "    ((\n",
        "        resize(b_train_data.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic','grapheme'], axis=1))/255).values.reshape(-1, 64, 64, 1), pd.get_dummies(\n",
        "        b_train_data['grapheme_root']).values, pd.get_dummies(b_train_data['vowel_diacritic']).values, pd.get_dummies(b_train_data['consonant_diacritic']).values,"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UAgD99nqFDu",
        "colab_type": "code",
        "outputId": "14196a3b-19d7-45ef-fb57-4002ad7db013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b_train_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50210, 32336)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ROZk_BGH2aT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = cv2.resize(df.iloc[:,1:].to_numpy().astype(int).reshape(137,236), (64,64))\n",
        "train_images.shape\n",
        "del df\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR6O4Ggil2X-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split((resize(\n",
        "        b_train_data.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic','grapheme'], axis=1))/255).values.reshape(-1, 64, 64, 1), pd.get_dummies(\n",
        "        b_train_data['grapheme_root']).values, pd.get_dummies(b_train_data['vowel_diacritic']).values, pd.get_dummies(b_train_data['consonant_diacritic']).values,"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCp7KjTOl2MR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K-UzjWT7nQu",
        "colab_type": "code",
        "outputId": "5f9b9beb-45b0-4ad2-fc57-eb6695e688da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "list_img_arr = []\n",
        "img_ids =[]\n",
        "for i in range(df.shape[0]):\n",
        "  if i%1000==0:\n",
        "    print(i)\n",
        "  img_arr = df.iloc[i,1:].to_numpy().astype(int).reshape(137,236)\n",
        "  #train_images = np.append(train_images, img_arr)\n",
        "  img_ids.append(df.iloc[i,0])\n",
        "  list_img_arr.append(img_arr)\n",
        "train_images = np.array(list_img_arr)\n",
        "print(train_images.shape)\n",
        "print(len(img_ids))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e94d99ddc76b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mimg_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m137\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m236\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;31m#train_images = np.append(train_images, img_arr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mimg_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   2092\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_label_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0msection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;31m# we have yielded a scalar ?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m     \u001b[0;31m# raise_missing is included for compat with the parent class signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_loc\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   2928\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2930\u001b[0;31m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_xs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2931\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mfast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;31m# result[blk.mgr_locs] = blk._slice((slice(None), loc))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_coerce_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36miget\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    367\u001b[0m         )\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kxlOrD-APyr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize handwritten image\n",
        "img_df = df.iloc[0:1,1:]\n",
        "img_arr = img_df.values\n",
        "img_arr = img_arr.reshape(137,236,1)\n",
        "img_arr.shape\n",
        "cv2.imwrite(\"hi.jpg\", img_arr)\n",
        "\n",
        "img = cv2.imread('hi.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "#img = cv2.resize(img, (225,225))\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcsvwsTD3IYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read train.csv\n",
        "df_train = pd.read_csv('/content/drive/My Drive/content/Bengali.AI Handwritten Grapheme Classification/Dataset/Dataset/train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yohj72FPCu0Q",
        "colab_type": "code",
        "outputId": "2d621ce3-947c-42dc-b19f-94afd8cc3d7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>grapheme_root</th>\n",
              "      <th>vowel_diacritic</th>\n",
              "      <th>consonant_diacritic</th>\n",
              "      <th>grapheme</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Train_0</td>\n",
              "      <td>15</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>ক্ট্রো</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Train_1</td>\n",
              "      <td>159</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>হ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Train_2</td>\n",
              "      <td>22</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>খ্রী</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Train_3</td>\n",
              "      <td>53</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>র্টি</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Train_4</td>\n",
              "      <td>71</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>থ্রো</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic grapheme\n",
              "0  Train_0             15                9                    5   ক্ট্রো\n",
              "1  Train_1            159                0                    0        হ\n",
              "2  Train_2             22                3                    5     খ্রী\n",
              "3  Train_3             53                2                    2     র্টি\n",
              "4  Train_4             71                9                    5     থ্রো"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqRR2SB7ASQJ",
        "colab_type": "code",
        "outputId": "6137b8d4-ef18-4c49-b93d-746f676189c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6w2TZXZyAZI-",
        "colab_type": "code",
        "outputId": "029012eb-ef8e-49cb-bbde-ffe908e711a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    }
  ]
}